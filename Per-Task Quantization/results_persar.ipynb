{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e408f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                               Method                  EM (%)  F1 (%)  N    \n",
      "----------------------------------  ----------------------  ------  ------  -----\n",
      "Qwen/Qwen2.5-7B-Instruct            PTQ4                    18.90   29.92   2048 \n",
      "Qwen/Qwen2.5-7B-Instruct            AWQ-OT                  12.80   25.20   1750 \n",
      "Qwen/Qwen2.5-7B-Instruct            AWQ_MIX                 10.46   24.34   10766\n",
      "Qwen/Qwen2.5-7B-Instruct            Clean                   11.43   21.90   2048 \n",
      "Qwen/Qwen2.5-7B-Instruct            INFO_VAR_MIX_s0         12.06   22.47   2048 \n",
      "Qwen/Qwen2.5-7B-Instruct            SOTA_AWQ_PerLayerMixed  11.91   24.06   2048 \n",
      "Qwen/Qwen2.5-7B-Instruct            SOTA_BNB_NF4            18.90   29.92   2048 \n",
      "Qwen/Qwen2.5-7B-Instruct            TAQ_PerLayerPerTask     12.65   25.49   2048 \n",
      "Qwen/Qwen3-4B-Instruct-2507         AWQ-OT                  9.81    24.33   2048 \n",
      "Qwen/Qwen3-4B-Instruct-2507         AWQ_MIX                 8.54    23.20   2048 \n",
      "Qwen/Qwen3-4B-Instruct-2507         PTQ4                    9.89    24.28   5227 \n",
      "Qwen/Qwen3-4B-Instruct-2507         SOTA_AWQ_All            9.18    23.89   2048 \n",
      "Qwen/Qwen3-4B-Instruct-2507         SOTA_AWQ_PerLayerMixed  7.59    22.81   10766\n",
      "Qwen/Qwen3-4B-Instruct-2507         SOTA_BNB_NF4            10.12   24.52   10766\n",
      "meta-llama/Llama-3.1-8B-Instruct    AWQ_final               55.66   62.92   2048 \n",
      "meta-llama/Llama-3.1-8B-Instruct    TAQ_PerLayerPerTask     57.91   65.14   2048 \n",
      "microsoft/Phi-4-mini-instruct       AWQ-OT                  0.73    4.54    2048 \n",
      "microsoft/Phi-4-mini-instruct       AWQ_MIX                 0.05    1.50    2048 \n",
      "mistralai/Mistral-7B-Instruct-v0.2  AWQ-OT                  18.90   34.90   2048 \n",
      "mistralai/Mistral-7B-Instruct-v0.2  AWQ_MIX                 17.48   33.75   2048 \n",
      "mistralai/Mistral-7B-Instruct-v0.2  AWQ_final               17.58   26.33   2048 \n",
      "mistralai/Mistral-7B-Instruct-v0.2  Clean                   18.90   35.18   2048 \n",
      "mistralai/Mistral-7B-Instruct-v0.2  INFO_VAR_MIX_s0         18.75   34.74   2048 \n",
      "mistralai/Mistral-7B-Instruct-v0.2  PTQ4                    19.87   35.76   2048 \n",
      "mistralai/Mistral-7B-Instruct-v0.2  SOTA_AWQ_PerLayerMixed  17.38   33.81   2048 \n",
      "mistralai/Mistral-7B-Instruct-v0.2  SOTA_BNB_NF4            19.87   35.76   2048 \n",
      "mistralai/Mistral-7B-Instruct-v0.2  TAQ_PerLayerPerTask     18.51   34.34   2048 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#===== form here its in jupiter\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import argparse\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "\n",
    "def _print_table(headers, rows):\n",
    "    widths = [len(str(h)) for h in headers]\n",
    "    for r in rows:\n",
    "        for i, c in enumerate(r):\n",
    "            widths[i] = max(widths[i], len(str(c)))\n",
    "    fmt = \"  \".join(\"{:\" + str(w) + \"s}\" for w in widths)\n",
    "    print(fmt.format(*headers))\n",
    "    print(\"  \".join(\"-\" * w for w in widths))\n",
    "    for r in rows:\n",
    "        print(fmt.format(*[str(c) for c in r]))\n",
    "\n",
    "\n",
    "def print_results_table(results_dir: str = os.path.join(\"models_and_tokenizers\", \"results\"), method: str | None = None) -> None:\n",
    "    \"\"\"\n",
    "    Print a compact results table combining summary.csv (if present) and any\n",
    "    '*_results.jsonl' files in the directory. Summary rows take precedence; any\n",
    "    (model, method) pairs missing from the summary are filled from JSONL.\n",
    "\n",
    "        Columns: Model | Method | EM (%) | F1 (%) | N\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(results_dir):\n",
    "        print(f\"[!] Results directory not found: {results_dir}\")\n",
    "        return\n",
    "\n",
    "    method_norm = method.lower() if isinstance(method, str) else None\n",
    "\n",
    "    # Accumulator keyed by (model, method)\n",
    "    rows_map: \"OrderedDict[tuple[str,str], tuple[str,str,str,str,str]]\" = OrderedDict()\n",
    "\n",
    "    # 1) Pull from summary.csv when present (preferred)\n",
    "    csv_path = os.path.join(results_dir, \"summary.csv\")\n",
    "    if os.path.isfile(csv_path):\n",
    "        try:\n",
    "            with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                rdr = csv.DictReader(f)\n",
    "                for r in rdr:\n",
    "                    meth = str(r.get(\"method\", \"\"))\n",
    "                    if method_norm and meth.lower() != method_norm:\n",
    "                        continue\n",
    "                    try:\n",
    "                        em = float(r.get(\"em\", 0.0) or 0.0) * 100.0\n",
    "                        f1 = float(r.get(\"f1\", 0.0) or 0.0) * 100.0\n",
    "                        n = int(r.get(\"n_examples\", 0) or 0)\n",
    "                    except Exception:\n",
    "                        em, f1, n = 0.0, 0.0, 0\n",
    "                    key = (str(r.get(\"model\", \"\")), meth)\n",
    "                    rows_map[key] = (\n",
    "                        key[0], key[1], f\"{em:.2f}\", f\"{f1:.2f}\", str(n)\n",
    "                    )\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Failed to read {csv_path}: {e}\")\n",
    "\n",
    "    # 2) Aggregate JSONL results and add any missing keys\n",
    "    jsonl_files = [fn for fn in os.listdir(results_dir) if fn.endswith(\"_results.jsonl\")]\n",
    "    agg = defaultdict(lambda: {\"em_sum\": 0.0, \"f1_sum\": 0.0, \"n\": 0})\n",
    "    for name in sorted(jsonl_files):\n",
    "        path = os.path.join(results_dir, name)\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    try:\n",
    "                        rec = json.loads(line)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    method_val = str(rec.get(\"method\", \"\"))\n",
    "                    if method_norm and method_val.lower() != method_norm:\n",
    "                        continue\n",
    "                    key = (str(rec.get(\"model\", \"\")), method_val)\n",
    "                    agg[key][\"em_sum\"] += float(rec.get(\"em\", 0.0))\n",
    "                    agg[key][\"f1_sum\"] += float(rec.get(\"f1\", 0.0))\n",
    "                    agg[key][\"n\"] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Skipping {path}: {e}\")\n",
    "\n",
    "    for key, d in sorted(agg.items()):\n",
    "        if key in rows_map:\n",
    "            # Already covered by summary.csv; keep the summary version\n",
    "            continue\n",
    "        n = max(d[\"n\"], 1)\n",
    "        em_pct = 100.0 * d[\"em_sum\"] / n\n",
    "        f1_pct = 100.0 * d[\"f1_sum\"] / n\n",
    "        rows_map[key] = (key[0], key[1], f\"{em_pct:.2f}\", f\"{f1_pct:.2f}\", str(d[\"n\"]))\n",
    "\n",
    "    rows = list(rows_map.values())\n",
    "    if rows:\n",
    "        headers = [\"Model\", \"Method\", \"EM (%)\", \"F1 (%)\", \"N\"]\n",
    "        _print_table(headers, rows)\n",
    "    else:\n",
    "        print(f\"[!] No results found in {results_dir}.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Print results table from model evaluation.\")\n",
    "    parser.add_argument(\n",
    "        \"--results-dir\",\n",
    "        type=str,\n",
    "        default=os.path.join(\"models_and_tokenizers\", \"results\"),\n",
    "        help=\"Directory containing results (default: models_and_tokenizers/results).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--method\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Optional filter by method (exact match, case-insensitive). Omit to show all methods.\",\n",
    "    )\n",
    "    # Use parse_known_args so it works inside Jupyter (ignores notebook's own flags)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    print_results_table(args.results_dir, method=args.method)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3a66f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested PCA coordinates (PC1, PC2) per task for selected layers:\n",
      "  Task=Trivia Layer= 0: (0.0485, -0.3452)\n",
      "  Task=Code   Layer= 0: (-0.0207, -0.0267)\n",
      "  Task=Math   Layer= 0: (0.0117, -0.2023)\n",
      "  Task=Trivia Layer= 8: (0.0275, -0.3047)\n",
      "  Task=Code   Layer= 8: (0.2710, 0.0811)\n",
      "  Task=Math   Layer= 8: (0.1461, -0.0523)\n",
      "  Task=Trivia Layer=16: (0.1290, 0.3419)\n",
      "  Task=Code   Layer=16: (-0.3552, 0.2158)\n",
      "  Task=Math   Layer=16: (-0.4433, 0.1342)\n",
      "  Task=Trivia Layer=24: (-0.0294, 0.2398)\n",
      "  Task=Code   Layer=24: (-0.1665, 0.1724)\n",
      "  Task=Math   Layer=24: (-0.1192, 0.0244)\n",
      "  [skip] layer 32 is out of range (0..31)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/amitlevi/work/Per-Task Quantization/viz_results/taq_pca_layers.png',\n",
       " '/home/amitlevi/work/Per-Task Quantization/viz_results/taq_layer_relevance_lines.png')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This script generates two motivation figures:\n",
    "# 1) PCA projection of per-layer, per-task semantic directions (synthetic demo)\n",
    "# 2) Layer-wise relevance/activation metric lines per task (synthetic demo)\n",
    "#\n",
    "# Replace the synthetic blocks with your real {d_i^k} and r_i when ready.\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.default_rng(7)\n",
    "\n",
    "# ----- Config -----\n",
    "num_layers = 32\n",
    "hidden_dim = 128\n",
    "tasks = [\"Trivia\", \"Code\", \"Math\"]\n",
    "\n",
    "# Prepare output directory for figures\n",
    "out_dir = os.path.join(os.getcwd(), \"viz_results\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# ----- Synthetic semantic directions d_i^k (unit vectors) -----\n",
    "# Create a base per-layer vector, then add task-specific offsets\n",
    "base_layer_vecs = rng.normal(size=(num_layers, hidden_dim))\n",
    "base_layer_vecs /= np.linalg.norm(base_layer_vecs, axis=1, keepdims=True)\n",
    "\n",
    "task_offsets = {t: rng.normal(scale=0.25, size=(num_layers, hidden_dim)) for t in tasks}\n",
    "\n",
    "dirs = {}\n",
    "for t in tasks:\n",
    "    D = base_layer_vecs + task_offsets[t] + rng.normal(scale=0.05, size=(num_layers, hidden_dim))\n",
    "    D /= np.linalg.norm(D, axis=1, keepdims=True)\n",
    "    dirs[t] = D  # shape: [num_layers, hidden_dim]\n",
    "\n",
    "# ----- PCA over all layers×tasks -----\n",
    "X = np.concatenate([dirs[t] for t in tasks], axis=0)  # [num_layers * T, hidden_dim]\n",
    "Xc = X - X.mean(axis=0, keepdims=True)\n",
    "U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
    "PCs = Xc @ Vt.T[:, :2]  # first two components\n",
    "explained = (S**2) / (S**2).sum()\n",
    "explained2 = explained[:2].sum()\n",
    "\n",
    "# Split back per task\n",
    "split_PCs = {}\n",
    "start = 0\n",
    "for t in tasks:\n",
    "    split_PCs[t] = PCs[start:start+num_layers]\n",
    "    start += num_layers\n",
    "\n",
    "# ----- Figure 1: PCA scatter labeled by layer, colored by task -----\n",
    "fig1 = plt.figure(figsize=(9, 5), dpi=140)\n",
    "for t in tasks:\n",
    "    pts = split_PCs[t]\n",
    "    plt.scatter(pts[:, 0], pts[:, 1], alpha=0.75, label=t, s=24)\n",
    "    # Light annotations for every 4th layer to avoid clutter\n",
    "    for i in range(0, num_layers, 4):\n",
    "        x, y = pts[i]\n",
    "        plt.text(x, y, f\"{i}\", fontsize=7, ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.title(f\"PCA of Semantic Directions by Layer and Task (var exp ~ {explained2*100:.1f}%)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend(title=\"Task\", frameon=True)\n",
    "plt.tight_layout()\n",
    "pca_path = os.path.join(out_dir, \"taq_pca_layers.png\")\n",
    "plt.savefig(pca_path, bbox_inches=\"tight\")\n",
    "plt.close(fig1)\n",
    "\n",
    "# ----- Synthetic layer-wise relevance r_i per task (higher = keep more bits) -----\n",
    "# Shape patterns to mimic different task-critical layers\n",
    "x = np.arange(num_layers)\n",
    "rel = {}\n",
    "rel[\"Trivia\"] = 0.6*np.exp(-((x-22)/6.5)**2) + 0.25*np.sin(x/3.5) + 0.2 + rng.normal(scale=0.03, size=num_layers)\n",
    "rel[\"Code\"]   = 0.55*np.exp(-((x-10)/5.0)**2) + 0.20*np.cos(x/4.0) + 0.18 + rng.normal(scale=0.03, size=num_layers)\n",
    "rel[\"Math\"]   = 0.50*np.exp(-((x-14)/4.5)**2) + 0.22*np.sin(x/4.4) + 0.16 + rng.normal(scale=0.03, size=num_layers)\n",
    "\n",
    "# Normalize to [0,1] for a clean axis\n",
    "for t in tasks:\n",
    "    v = rel[t]\n",
    "    v = (v - v.min()) / (v.max() - v.min())\n",
    "    rel[t] = v\n",
    "\n",
    "# ----- Figure 2: Lines over layers (one line per task) -----\n",
    "fig2 = plt.figure(figsize=(9, 5), dpi=140)\n",
    "for t in tasks:\n",
    "    plt.plot(x, rel[t], marker=\"o\", linewidth=1.75, markersize=3, label=t)\n",
    "\n",
    "plt.title(\"Layer-wise Relevance by Task (higher ⇒ allocate more bits)\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Relevance / Activation-variance proxy (normalized)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(title=\"Task\", ncols=len(tasks))\n",
    "plt.tight_layout()\n",
    "lines_path = os.path.join(out_dir, \"taq_layer_relevance_lines.png\")\n",
    "plt.savefig(lines_path, bbox_inches=\"tight\")\n",
    "plt.close(fig2)\n",
    "\n",
    "# ----- Print PCA coords for specific layers -----\n",
    "request_layers = [0, 8, 16, 24, 32]\n",
    "print(\"Requested PCA coordinates (PC1, PC2) per task for selected layers:\")\n",
    "for idx in request_layers:\n",
    "    if not (0 <= idx < num_layers):\n",
    "        print(f\"  [skip] layer {idx} is out of range (0..{num_layers-1})\")\n",
    "        continue\n",
    "    for t in tasks:\n",
    "        pt = split_PCs[t][idx]\n",
    "        print(f\"  Task={t:6s} Layer={idx:2d}: ({pt[0]:.4f}, {pt[1]:.4f})\")\n",
    "\n",
    "pca_path, lines_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98b64151",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m     mdl_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m hf_token\n\u001b[1;32m     45\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_ID, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtok_kwargs)\n\u001b[0;32m---> 46\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmdl_kwargs\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(DEVICE)\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# padding hygiene (Llama-style)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/llm/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:558\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[1;32m    557\u001b[0m     class_ref \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mauto_map[\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m]\n\u001b[0;32m--> 558\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m     _ \u001b[38;5;241m=\u001b[39m hub_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda/envs/llm/lib/python3.10/site-packages/transformers/dynamic_module_utils.py:570\u001b[0m, in \u001b[0;36mget_class_from_dynamic_module\u001b[0;34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[1;32m    558\u001b[0m final_module \u001b[38;5;241m=\u001b[39m get_cached_module_file(\n\u001b[1;32m    559\u001b[0m     repo_id,\n\u001b[1;32m    560\u001b[0m     module_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m     repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    569\u001b[0m )\n\u001b[0;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_class_in_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/llm/lib/python3.10/site-packages/transformers/dynamic_module_utils.py:267\u001b[0m, in \u001b[0;36mget_class_in_module\u001b[0;34m(class_name, module_path, force_reload)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# reload in both cases, unless the module is already imported and the hash hits\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__transformers_module_hash__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m module_hash:\n\u001b[0;32m--> 267\u001b[0m     \u001b[43mmodule_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m     module\u001b[38;5;241m.\u001b[39m__transformers_module_hash__ \u001b[38;5;241m=\u001b[39m module_hash\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, class_name)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-mini-instruct/5a149550068a1eb93398160d8953f5f56c3603e9/modeling_phi3.py:35\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     BaseModelOutputWithPast,\n\u001b[1;32m     30\u001b[0m     CausalLMOutputWithPast,\n\u001b[1;32m     31\u001b[0m     SequenceClassifierOutputWithPast,\n\u001b[1;32m     32\u001b[0m     TokenClassifierOutput,\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_rope_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ROPE_INIT_FUNCTIONS\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessing_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Unpack\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     38\u001b[0m     LossKwargs,\n\u001b[1;32m     39\u001b[0m     add_code_sample_docstrings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     replace_return_docstrings,\n\u001b[1;32m     44\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda/envs/llm/lib/python3.10/site-packages/transformers/modeling_utils.py:69\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdpa_attention\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sdpa_attention_forward\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_parallel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     66\u001b[0m     SUPPORTED_TP_STYLES,\n\u001b[1;32m     67\u001b[0m     shard_and_distribute_module,\n\u001b[1;32m     68\u001b[0m )\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LOSS_MAPPING\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     Conv1D,\n\u001b[1;32m     72\u001b[0m     apply_chunking_to_forward,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m     prune_linear_layer,\n\u001b[1;32m     78\u001b[0m )\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoHfQuantizer, HfQuantizer\n",
      "File \u001b[0;32m~/miniconda/envs/llm/lib/python3.10/site-packages/transformers/loss/loss_utils.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BCEWithLogitsLoss, MSELoss\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_deformable_detr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeformableDetrForObjectDetectionLoss, DeformableDetrForSegmentationLoss\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_for_object_detection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ForObjectDetectionLoss, ForSegmentationLoss\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_grounding_dino\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GroundingDinoForObjectDetectionLoss\n",
      "File \u001b[0;32m~/miniconda/envs/llm/lib/python3.10/site-packages/transformers/loss/loss_deformable_detr.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m center_to_corners_format\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_scipy_available\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_for_object_detection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     HungarianMatcher,\n\u001b[1;32m      8\u001b[0m     ImageLoss,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     sigmoid_focal_loss,\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda/envs/llm/lib/python3.10/site-packages/transformers/image_transforms.py:21\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional, Union\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     ChannelDimension,\n\u001b[1;32m     23\u001b[0m     ImageInput,\n\u001b[1;32m     24\u001b[0m     get_channel_dimension_axis,\n\u001b[1;32m     25\u001b[0m     get_image_size,\n\u001b[1;32m     26\u001b[0m     infer_channel_dimension_format,\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExplicitEnum, TensorType, is_jax_tensor, is_tf_tensor, is_torch_tensor\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     is_flax_available,\n\u001b[1;32m     31\u001b[0m     is_tf_available,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     requires_backends,\n\u001b[1;32m     35\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda/envs/llm/lib/python3.10/site-packages/transformers/image_utils.py:64\u001b[0m\n\u001b[1;32m     61\u001b[0m     PILImageResampling \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torchvision_available():\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m io \u001b[38;5;28;01mas\u001b[39;00m torchvision_io\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InterpolationMode\n\u001b[1;32m     67\u001b[0m     pil_torch_interpolation_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     68\u001b[0m         PILImageResampling\u001b[38;5;241m.\u001b[39mNEAREST: InterpolationMode\u001b[38;5;241m.\u001b[39mNEAREST,\n\u001b[1;32m     69\u001b[0m         PILImageResampling\u001b[38;5;241m.\u001b[39mBOX: InterpolationMode\u001b[38;5;241m.\u001b[39mBOX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m         PILImageResampling\u001b[38;5;241m.\u001b[39mLANCZOS: InterpolationMode\u001b[38;5;241m.\u001b[39mLANCZOS,\n\u001b[1;32m     74\u001b[0m     }\n",
      "File \u001b[0;32m~/miniconda/envs/llm/lib/python3.10/site-packages/torchvision/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/llm/lib/python3.10/site-packages/torchvision/_meta_registrations.py:26\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129;43m@register_meta\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroi_align\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43mmeta_roi_align\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maligned\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrois must have shape as Tensor[K, 5]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/llm/lib/python3.10/site-packages/torchvision/_meta_registrations.py:18\u001b[0m, in \u001b[0;36mregister_meta.<locals>.wrapper\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(fn):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextension\u001b[49m\u001b[38;5;241m.\u001b[39m_has_ops():\n\u001b[1;32m     19\u001b[0m         get_meta_lib()\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mtorchvision, op_name), overload_name), fn)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# Per-layer \"Prompt Entropy\" for Llama-3.1-8B-Instruct across task types\n",
    "# - Metric from \"Layer by Layer: Uncovering Hidden Representations in Language Models\"\n",
    "#   (matrix-based entropy of K = Z Z^T; see §§3.2–3.3). Higher => richer, less-compressed features.\n",
    "#   Paper: https://arxiv.org/abs/2502.02013\n",
    "\n",
    "# %%capture\n",
    "\n",
    "# %%\n",
    "import os, math, random, json, gc\n",
    "from typing import List, Dict, Tuple\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "MODEL_ID = \"microsoft/Phi-4-mini-instruct\"   # requires gated access\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16\n",
    "\n",
    "# scoring params\n",
    "MAX_TEXTS_PER_TASK = 128          # keep modest for speed; raise if you have time\n",
    "MAX_TOKENS = 512                  # cap per text for scoring forward\n",
    "BATCH_SIZE = 4\n",
    "RESERVOIR_TOKENS_PER_LAYER = 256  # subsample tokens per layer to bound eigendecomp size\n",
    "SEED = 7\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# Load model + tokenizer\n",
    "# -----------------------------\n",
    "hf_token = os.environ.get(\"HUGGING_FACE_HUB_TOKEN\", None)\n",
    "tok_kwargs = dict(trust_remote_code=True, use_fast=False)\n",
    "mdl_kwargs = dict(torch_dtype=DTYPE, trust_remote_code=True, low_cpu_mem_usage=True, device_map=None)\n",
    "\n",
    "if hf_token:\n",
    "    tok_kwargs[\"token\"] = hf_token\n",
    "    mdl_kwargs[\"token\"] = hf_token\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, **tok_kwargs)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, **mdl_kwargs\n",
    ").to(DEVICE).eval()\n",
    "# padding hygiene (Llama-style)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: #layers\n",
    "# -----------------------------\n",
    "def get_num_layers_llama(m):\n",
    "    if hasattr(m, \"model\") and hasattr(m.model, \"layers\"):\n",
    "        return len(m.model.layers)\n",
    "    if hasattr(m, \"model\") and hasattr(m.model, \"decoder\") and hasattr(m.model.decoder, \"layers\"):\n",
    "        return len(m.model.decoder.layers)\n",
    "    return 0\n",
    "\n",
    "N_LAYERS = get_num_layers_llama(model)\n",
    "print(f\"Model: {MODEL_ID} | Layers: {N_LAYERS} | Device: {DEVICE}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Datasets & prompt builders\n",
    "# -----------------------------\n",
    "def load_texts_for_task(task: str, k: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of *input-only* texts (no labels) to score hidden states.\n",
    "    \"\"\"\n",
    "    if task == \"trivia\":\n",
    "        ds = load_dataset(\"trivia_qa\", \"rc.nocontext\", split=\"validation\")\n",
    "        idx = list(range(len(ds))); random.shuffle(idx); idx = idx[:k]\n",
    "        texts = [f\"Question: {ds[i]['question']}\\nAnswer:\" for i in idx]\n",
    "        return texts\n",
    "\n",
    "    if task == \"math\":\n",
    "        # gsm8k 'test' is smaller and public\n",
    "        ds = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
    "        idx = list(range(len(ds))); random.shuffle(idx); idx = idx[:k]\n",
    "        texts = [f\"Solve the math problem step by step.\\nProblem: {ds[i]['question']}\\nAnswer:\" for i in idx]\n",
    "        return texts\n",
    "\n",
    "    if task == \"code\":\n",
    "        # mbpp has natural-language prompts for code-gen\n",
    "        try:\n",
    "            ds = load_dataset(\"mbpp\", split=\"test\")\n",
    "        except Exception:\n",
    "            # fallback to train if test missing\n",
    "            ds = load_dataset(\"mbpp\", split=\"train\")\n",
    "        idx = list(range(len(ds))); random.shuffle(idx); idx = idx[:k]\n",
    "        def to_prompt(ex):\n",
    "            # Some variants store 'text', others ('prompt','text')\n",
    "            q = ex.get(\"text\") or ex.get(\"prompt\") or ex.get(\"task_id\") or \"\"\n",
    "            return f\"Write a Python function as specified.\\nTask: {q}\\nCode:\"\n",
    "        texts = [to_prompt(ds[i]) for i in idx]\n",
    "        return texts\n",
    "\n",
    "    raise ValueError(f\"Unknown task: {task}\")\n",
    "\n",
    "TASKS = [\"trivia\", \"math\", \"code\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Metric: Prompt Entropy (matrix-based Shannon entropy on Gram of token states)\n",
    "# Ref: §3.2–3.3 'Matrix-Based Entropy' and 'Prompt Entropy' in the paper.\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def prompt_entropy_per_layer(\n",
    "    texts: List[str],\n",
    "    model,\n",
    "    tokenizer,\n",
    "    max_tokens: int,\n",
    "    batch_size: int,\n",
    "    reservoir_per_layer: int,\n",
    "    device: str\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Compute *average* prompt entropy across a set of texts for each transformer layer.\n",
    "    Implementation:\n",
    "      - Run with output_hidden_states=True to get H_l \\in R^{B,T,D}\n",
    "      - For each layer, flatten to N x D (N=B*T), reservoir-sample up to R rows,\n",
    "        center, build Gram G = X X^T / N, eigenvalues -> p, entropy H = -sum p log p.\n",
    "    Returns:\n",
    "      List of length n_layers with the mean entropy over texts/batches.\n",
    "    \"\"\"\n",
    "    nL = get_num_layers_llama(model)\n",
    "    sums = [0.0 for _ in range(nL)]\n",
    "    counts = [0 for _ in range(nL)]\n",
    "\n",
    "    # simple reservoir per layer\n",
    "    def entropy_from_hidden(X: torch.Tensor) -> float:\n",
    "        # X: [N, D] float16/32 on device\n",
    "        if X.shape[0] < 2:\n",
    "            return 0.0\n",
    "        # reservoir sample up to R rows\n",
    "        N = X.shape[0]\n",
    "        R = min(reservoir_per_layer, N)\n",
    "        if R < N:\n",
    "            idx = torch.randint(0, N, (R,), device=X.device)\n",
    "            X = X.index_select(0, idx)\n",
    "        # center (token-wise)\n",
    "        X = X.to(torch.float32)\n",
    "        X = X - X.mean(dim=0, keepdim=True)\n",
    "        # Gram and eigenvalues\n",
    "        # scale by R to keep magnitudes stable\n",
    "        G = (X @ X.T) / max(1, X.shape[0])\n",
    "        # symmetric -> use eigvalsh\n",
    "        evals = torch.linalg.eigvalsh(G).clamp(min=0.0)\n",
    "        s = float(evals.sum().item())\n",
    "        if s <= 0:\n",
    "            return 0.0\n",
    "        p = (evals / s).double()\n",
    "        H = float((-(p * (p + 1e-12).log())).sum().item())  # natural log\n",
    "        return H\n",
    "\n",
    "    # batching over texts\n",
    "    for start in range(0, len(texts), batch_size):\n",
    "        batch = texts[start:start+batch_size]\n",
    "        enc = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_tokens).to(device)\n",
    "        out = model(**enc, output_hidden_states=True, use_cache=False)\n",
    "        hs = out.hidden_states  # tuple length nL+1 (0=embeddings)\n",
    "        for li in range(1, len(hs)):  # 1..nL\n",
    "            H = hs[li]                       # [B, T, D]\n",
    "            X = H.reshape(-1, H.shape[-1])   # [N, D]\n",
    "            val = entropy_from_hidden(X)\n",
    "            sums[li-1] += val\n",
    "            counts[li-1] += 1\n",
    "        # free a bit\n",
    "        del out, hs, enc\n",
    "        torch.cuda.empty_cache() if device.startswith(\"cuda\") else None\n",
    "\n",
    "    # mean across batches\n",
    "    means = [ (sums[i] / max(1, counts[i])) for i in range(nL) ]\n",
    "    return means\n",
    "\n",
    "# -----------------------------\n",
    "# Run scoring for each task\n",
    "# -----------------------------\n",
    "layer_to_series: Dict[str, List[float]] = {}\n",
    "for task in TASKS:\n",
    "    print(f\"[Scoring] {task} …\")\n",
    "    texts = load_texts_for_task(task, MAX_TEXTS_PER_TASK)\n",
    "    vals = prompt_entropy_per_layer(\n",
    "        texts, model, tokenizer,\n",
    "        max_tokens=MAX_TOKENS, batch_size=BATCH_SIZE,\n",
    "        reservoir_per_layer=RESERVOIR_TOKENS_PER_LAYER,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    layer_to_series[task] = vals\n",
    "\n",
    "# -----------------------------\n",
    "# Normalize per-task (for clean overlay)\n",
    "# -----------------------------\n",
    "def znormalize(xs: List[float]) -> List[float]:\n",
    "    m = sum(xs)/len(xs)\n",
    "    v = sum((x-m)*(x-m) for x in xs)/max(1, len(xs)-1)\n",
    "    s = math.sqrt(v + 1e-12)\n",
    "    return [(x - m) / s for x in xs]\n",
    "\n",
    "normed = {k: znormalize(v) for k, v in layer_to_series.items()}\n",
    "\n",
    "# -----------------------------\n",
    "# Plot: one line per task\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(9, 5))\n",
    "xs = list(range(1, N_LAYERS + 1))\n",
    "for task in TASKS:\n",
    "    plt.plot(xs, normed[task], label=task)  # (no explicit colors per house style)\n",
    "\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Prompt Entropy (z-score per task)\")\n",
    "plt.title(\"Layer-wise Prompt Entropy (matrix-based) — Llama-3.1-8B-Instruct\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# (Optional) Save raw numbers\n",
    "# -----------------------------\n",
    "out = {\n",
    "    \"model\": MODEL_ID,\n",
    "    \"metric\": \"prompt_entropy (matrix-based)\",\n",
    "    \"layers\": N_LAYERS,\n",
    "    \"params\": {\n",
    "        \"max_texts_per_task\": MAX_TEXTS_PER_TASK,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"reservoir_tokens_per_layer\": RESERVOIR_TOKENS_PER_LAYER,\n",
    "        \"seed\": SEED,\n",
    "    },\n",
    "    \"series\": layer_to_series,\n",
    "}\n",
    "with open(\"layer_prompt_entropy_llama31_8b_instruct.json\", \"w\") as f:\n",
    "    json.dump(out, f, indent=2)\n",
    "print(\"Saved raw metric to layer_prompt_entropy_llama31_8b_instruct.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
